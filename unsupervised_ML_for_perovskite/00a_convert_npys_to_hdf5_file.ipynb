{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c246c62-1121-4a6b-a094-4ab4f5b5afbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/work7/workspace/scratch/rk3078-WorkSpace04a/ExperimentalData/insitu/01_croppedData\n"
     ]
    }
   ],
   "source": [
    "cd /pfs/work7/workspace/scratch/rk3078-WorkSpace04a/ExperimentalData/insitu/01_croppedData/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c2e1a54-a6e0-4852-a06e-72a9bc79fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from pandas import HDFStore\n",
    "\n",
    "# path to dataset saved as npy-files\n",
    "dataset_path_npys=\"preprocessed/\"\n",
    "\n",
    "\n",
    "# specify path and filename where to save dataset as hdf5 \n",
    "savepath='hdf5/'\n",
    "dataset_saveas_filename = 'dataset.hdf5'\n",
    "\n",
    "if not os.path.exists(savepath):\n",
    "    os.makedirs(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "407d3def-59e3-4f8e-a8d8-fa28fdee8e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if hdf5 already exsits. if yes, delete it\n",
    "if os.path.exists(savepath+ dataset_saveas_filename):\n",
    "    os.remove(savepath+ dataset_saveas_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b9724-a7a6-4b8f-bf33-8b1943dad7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:36<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:20<00:00,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# open h5py file\n",
    "with h5py.File(savepath+ dataset_saveas_filename, 'w') as f:    \n",
    "    \n",
    "    subset_info = sorted(glob.glob(dataset_path_npys  +'*'))\n",
    "    \n",
    "    # iterate through the two folders of the subsets \"test\" and \"train\"\n",
    "    for subset_iter in range(len(subset_info)):\n",
    "\n",
    "        # create group for current subset in the h5py file\n",
    "        subset_path=subset_info[subset_iter]+\"/\"\n",
    "        subsetID=subset_path.split(\"/\")[-2]\n",
    "        f.create_group(subsetID)\n",
    "        print(subsetID)\n",
    "        \n",
    "        \n",
    "        # get list of all substrates in current subset and iterate through the substrates\n",
    "        substrate_info = sorted(glob.glob(subset_path  +'A*'))\n",
    "        for substrate_iter in tqdm(range(len(substrate_info))):\n",
    "            \n",
    "            # create group for current substrate in current subset in the h5py file\n",
    "            substrate_path=substrate_info[substrate_iter]         \n",
    "            subID=substrate_path[-3:]\n",
    "            f.create_group(subsetID+\"/\"+subID)\n",
    "            \n",
    "            # get list of all patches in current substrate and iterate through the patches\n",
    "            patch_info = sorted(glob.glob(substrate_path  +'/*'))\n",
    "            for patch_iter in range(len(patch_info)):\n",
    "\n",
    "                patch_path=patch_info[patch_iter]\n",
    "                #print(patch_path)\n",
    "                patchID=patch_path[-6:-4]\n",
    "                data_to_write=np.load(patch_path).astype('uint16')\n",
    "\n",
    "                # create h5py dataset with data of current patch in the corresponding group of subset and substrate\n",
    "                f.create_dataset(subsetID+\"/\"+subID+\"/\"+patchID, data= data_to_write)\n",
    " \n",
    "    # create empty groups to save cross-validation split information later on \n",
    "    f.create_group(\"train/cv_splits_5fold\")\n",
    "    f.create_group(\"train/cv_splits_5fold/fold0\")\n",
    "    f.create_group(\"train/cv_splits_5fold/fold1\")\n",
    "    f.create_group(\"train/cv_splits_5fold/fold2\")\n",
    "    f.create_group(\"train/cv_splits_5fold/fold3\")\n",
    "    f.create_group(\"train/cv_splits_5fold/fold4\")\n",
    "    \n",
    "# read-in csv-files and save each in pandas dataframe\n",
    "labels_test=pd.read_csv(dataset_path_npys+\"test/labels.csv\")\n",
    "labels_train=pd.read_csv(dataset_path_npys+\"train/labels.csv\")\n",
    "\n",
    "fold0_train=pd.read_csv(dataset_path_npys+\"train/cv_splits_5fold/fold0/train.csv\")\n",
    "fold0_val=pd.read_csv(dataset_path_npys+\"train/cv_splits_5fold/fold0/val.csv\")\n",
    "fold1_train=pd.read_csv(dataset_path_npys+\"train/cv_splits_5fold/fold1/train.csv\")\n",
    "fold1_val=pd.read_csv(dataset_path_npys+\"train/cv_splits_5fold/fold1/val.csv\")\n",
    "fold2_train=pd.read_csv(dataset_path_npys+\"train/cv_splits_5fold/fold2/train.csv\")\n",
    "fold2_val=pd.read_csv(dataset_path_npys+\"train/cv_splits_5fold/fold2/val.csv\")\n",
    "fold3_train=pd.read_csv(dataset_path_npys+\"train/cv_splits_5fold/fold3/train.csv\")\n",
    "fold3_val=pd.read_csv(dataset_path_npys+\"train/cv_splits_5fold/fold3/val.csv\")\n",
    "fold4_train=pd.read_csv(dataset_path_npys+\"train/cv_splits_5fold/fold4/train.csv\")\n",
    "fold4_val=pd.read_csv(dataset_path_npys+\"train/cv_splits_5fold/fold4/val.csv\")\n",
    "\n",
    "# use HDFStore function to store pandas dataframes in the corresponding groups which had been created before \n",
    "hdf = HDFStore(savepath+ dataset_saveas_filename)\n",
    "hdf.put('test/labels',labels_test)\n",
    "hdf.put('train/labels',labels_train)\n",
    "\n",
    "hdf.put('train/cv_splits_5fold/fold0/train',fold0_train)\n",
    "hdf.put('train/cv_splits_5fold/fold0/val',fold0_val)\n",
    "hdf.put('train/cv_splits_5fold/fold1/train',fold1_train)\n",
    "hdf.put('train/cv_splits_5fold/fold1/val',fold1_val)\n",
    "hdf.put('train/cv_splits_5fold/fold2/train',fold2_train)\n",
    "hdf.put('train/cv_splits_5fold/fold2/val',fold2_val)\n",
    "hdf.put('train/cv_splits_5fold/fold3/train',fold3_train)\n",
    "hdf.put('train/cv_splits_5fold/fold3/val',fold3_val)\n",
    "hdf.put('train/cv_splits_5fold/fold4/train',fold4_train)\n",
    "hdf.put('train/cv_splits_5fold/fold4/val',fold4_val)\n",
    "hdf.close()\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7920f49-f699-4ea8-a7c2-98c3b9cc1751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95861877-188a-44bb-bdbb-7b2550a0f2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
